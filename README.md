# Rocket Landing on Rough Surfaces using Reinforcement Learning

A comprehensive comparison of Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) algorithms for autonomous rocket landing in both flat and uneven terrain scenarios.

## ğŸš€ Overview

This project implements and compares two state-of-the-art reinforcement learning algorithms for autonomous rocket landing control. The study demonstrates SAC's superior sample efficiency and performance compared to PPO, achieving 8.4Ã— faster training and 2.3Ã— higher peak rewards.

## ğŸ“Š Key Results

- **Sample Efficiency**: SAC requires 8.4Ã— fewer timesteps (300K vs 2.4M)
- **Training Speed**: SAC trains in 21 minutes vs PPO's 8 hours
- **Peak Performance**: SAC achieves 2.3Ã— higher single-episode rewards (+690 vs +305)
- **Terrain Robustness**: SAC retains 82.7% performance, PPO retains 67.8%
- **Action Precision**: Continuous actions enable smoother control

## ğŸ—ï¸ Project Structure

```
Rocket_landing/
â”œâ”€â”€ PPO.py                          # PPO algorithm implementation
â”œâ”€â”€ train.py                        # PPO training script
â”œâ”€â”€ test.py                         # Testing script
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ plain_sac/                     # SAC implementation for plain terrain
â”œâ”€â”€ uneven_sac/                     # SAC implementation for uneven terrain
â”œâ”€â”€ final_uneven_terrain/          # PPO uneven terrain training
â”œâ”€â”€ training_graphs/               # Generated plots and visualizations
â””â”€â”€ PPO_logs/                      # Training logs and checkpoints
```

## ğŸ¯ Algorithms

### PPO (Proximal Policy Optimization)
- **Type**: On-policy
- **Actions**: Discrete (9 combinations)
- **Strengths**: Stable, low variance
- **Use Case**: Final validation, safety-critical applications

### SAC (Soft Actor-Critic)
- **Type**: Off-policy
- **Actions**: Continuous
- **Strengths**: Sample efficient, high performance
- **Use Case**: Rapid development, prototyping

## ğŸŒ Environments

- **Plain Surface**: Flat landing pad for baseline testing
- **Uneven Terrain**: Procedurally generated with Â±25m variation and 5 features

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- PyTorch 1.9+
- Stable-Baselines3 2.0+
- NumPy, Matplotlib

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/Abhiram678/Rocket_landing.git
cd Rocket_landing
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Training

#### Train PPO (Plain Surface)
```bash
python train.py
```

#### Train PPO (Uneven Terrain)
```bash
cd final_uneven_terrain
python train.py
```

#### Train SAC (Plain Surface)
```bash
cd plain_sac
python train_sac.py
```

#### Train SAC (Uneven Terrain)
```bash
cd uneven_sac
python train_sac_uneven.py
```

### Testing
```bash
python test.py
```

### Generate Results
```bash
# SAC Plain
cd plain_sac
python plot_sac_final.py

# SAC Uneven
cd uneven_sac
python plot_sac_uneven_final.py

# Comparison plots
cd uneven_sac
python plot_premium_comparison.py
```

## ğŸ“ˆ Environment Details

### State Space
8-dimensional continuous observations:
- Position (x, y) in meters
- Velocity (vx, vy) in m/s
- Body angle Î¸ in radians
- Angular velocity Î¸Ì‡ in rad/s
- Timestep counter t
- Gimbal angle Ï† in radians

### Action Space
- **PPO**: 9 discrete actions (3 thrust levels Ã— 3 gimbal angles)
- **SAC**: Continuous actions (thrust: 0.2g-2.0g, angle: Â±30Â°)

### Physics Parameters
- Rocket height: 50m
- Gravity: 9.8 m/sÂ²
- Simulation timestep: 0.05s
- World bounds: x âˆˆ [-300, 300]m, y âˆˆ [-30, 570]m

## ğŸ”§ Hyperparameters

### PPO Configuration
- Learning rate: 3Ã—10â»â´
- Discount factor: 0.99
- GAE parameter: 0.95
- Batch size: 4000 timesteps
- Optimization epochs: 80

### SAC Configuration
- Learning rate: 3Ã—10â»â´
- Discount factor: 0.99
- Replay buffer: 1M transitions
- Batch size: 256
- Target update rate: 0.005

## ğŸ“Š Performance Metrics

| Metric | PPO Plain | PPO Uneven | SAC Plain | SAC Uneven |
|--------|-----------|------------|-----------|------------|
| Training Time | 8 hours | 8 hours | 21 min | 25 min |
| Peak Reward | +305.2 | +215.8 | +690.34 | +896.42 |
| Convergence | 1.5M steps | 1.8M steps | 250K steps | 200K steps |
| Success Rate | 65% | 45% | 70% | 60% |

## ğŸ“ Academic Context

This project implements a comprehensive case study comparing PPO and SAC algorithms for autonomous rocket landing. The research demonstrates:

- Superior sample efficiency of off-policy learning
- Impact of action space design on control precision
- Terrain robustness evaluation through procedural generation
- Practical feasibility for resource-constrained development

## ğŸ“„ Citation

```bibtex
@misc{rocket-landing-rl-2025,
  author = {Suravarapu Abhiram},
  title = {Rocket Landing on Rough Surfaces using Reinforcement Learning},
  year = {2025},
  url = {https://github.com/Abhiram678/Rocket_landing}
}
```

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or feedback, please open an issue in this repository.

---

**Made with â¤ï¸ for autonomous aerospace control** ğŸš€